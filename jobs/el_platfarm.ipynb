{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hvac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9dee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from awswrangler import s3\n",
    "from botocore.exceptions import HTTPClientError\n",
    "from pandas.core.series import Series\n",
    "from pandas import Timestamp\n",
    "from sqlalchemy import create_engine\n",
    "from pandas import (\n",
    "    read_sql,\n",
    "    to_datetime,\n",
    "    DataFrame,\n",
    ")\n",
    "from pandas import read_parquet as pd_read_parquet\n",
    "import json\n",
    "import boto3\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce\n",
    "import io\n",
    "import traceback\n",
    "import sys\n",
    "from numpy import dtype\n",
    "from sqlalchemy.engine.base import Engine\n",
    "import hvac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "befaad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"s3\": {\n",
    "        \"bucket_name\": \"teste-emr\",\n",
    "        \"base_path\": \"platfarm\",\n",
    "        \"table_prefix\": \"platfarm_\",\n",
    "    },\n",
    "    \"platfarm\": {\n",
    "        # S3 Path to database credentials file\n",
    "        \"auth\": \"platfarm_aurora_prd\",\n",
    "        # Configs for table ingestion\n",
    "        \"tables\": \"etc/platfarm_tables_config.json\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3918803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_valt(path):\n",
    "    client = hvac.Client(\n",
    "        url='https://vault-dados.portalfarm.com.br/',\n",
    "        token='hvs.due0AhBPNqvz2a3laFSHzc61',\n",
    "    )\n",
    "    #print(client.is_authenticated())\n",
    "    #quit()\n",
    "\n",
    "    path1 = path.replace(\".json\", \"\")\n",
    "    path = path1.replace(\"https://vault-dados.portalfarm.com.br/ui/vault/secrets/farm-bi-aws-glue-configs/show/platfarm_aurora_prd\",\"https://vault-dados.portalfarm.com.br/farm-bi-aws-glue-configs/platfarm_aurora_prd\")\n",
    "    # Reading a secret\n",
    "    result = client.secrets.kv.v2.read_secret_version(mount_point='farm-bi-aws-glue-configs', path=path)\n",
    "    #print('base_url = ' + result['data']['data']['base_url'])\n",
    "    #print('token = ' + result['data']['data']['token'])\n",
    "    return result['data']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30d7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dates(data, date_cols: list):\n",
    "    df = data.copy()\n",
    "    for column in date_cols:\n",
    "        try:\n",
    "            assert column in df.columns\n",
    "            df[column] = to_datetime(df[column].astype(str), errors=\"coerce\")\n",
    "        except AssertionError:\n",
    "            print(f\"Column {column} not in dataframe\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d905078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_configs(key):\n",
    "    bucket = \"teste-emr\"\n",
    "    key = f\"configs/{key}\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    response = s3.Object(bucket, key).get()\n",
    "    if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "        body = response[\"Body\"]\n",
    "        json_data = body.read()\n",
    "        json_dict = json.loads(json_data.decode(\"utf8\"))\n",
    "        return json_dict\n",
    "    else:\n",
    "        raise HTTPClientError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a2934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_path_exists(path, existing_paths):\n",
    "    checks = [path in x for x in existing_paths]\n",
    "    return any(checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb2d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S3Client:\n",
    "    resource = boto3.resource(\"s3\")\n",
    "    client = boto3.client(\"s3\")\n",
    "\n",
    "    def exists(self, bucket: str, prefix: str) -> bool:\n",
    "        object_listing: Dict[str, Any] = self.client.list_objects_v2(\n",
    "            Bucket=bucket, Prefix=prefix\n",
    "        )\n",
    "        key_count: int = object_listing[\"KeyCount\"]\n",
    "        return key_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71694f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S3Object:\n",
    "    client: S3Client\n",
    "    bucket: str\n",
    "    key: str\n",
    "\n",
    "    def get_path(self, *paths: str) -> str:\n",
    "        basepath = f\"s3://{self.bucket}/{self.key}\"\n",
    "        reducer = lambda x, y: x + \"/\" + y\n",
    "        full_path = basepath + reduce(reducer, paths, \"\")\n",
    "        return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de4f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class S3Sink(S3Object):\n",
    "    format: str = \"parquet\"\n",
    "    mode: str = \"append\"\n",
    "    dtype: Optional[Dict[str, str]] = None\n",
    "    partition_cols: Optional[List[str]] = None\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        return self.client.exists(self.bucket, self.key)\n",
    "\n",
    "    def detect_dtypes(self, obj:DataFrame):\n",
    "        dtype_mapping = {\n",
    "            \"int64\": \"bigint\",\n",
    "            \"int32\": \"int\",\n",
    "            \"datetime64[ns]\": \"timestamp\",\n",
    "            \"object\": \"string\",\n",
    "            \"string\": \"string\",\n",
    "            \"float64\": \"double\",\n",
    "            \"float32\": \"double\",\n",
    "        }\n",
    "        dtypes_dict:Dict[str, dtype] = obj.dtypes.to_dict()\n",
    "        dtypes = {\n",
    "            k: dtypes_dict[k].name if dtypes_dict[k].name != \"object\" else \"string\"\n",
    "            for k in dtypes_dict\n",
    "        }\n",
    "        dtypes_mapped = {k: dtype_mapping[v.lower()] for k, v in list(dtypes.items())}\n",
    "        return dtypes_mapped\n",
    "\n",
    "\n",
    "    def _write_parquet(self, frame: DataFrame) -> None:\n",
    "        dtype = self.detect_dtypes(frame)\n",
    "        s3.to_parquet(\n",
    "            frame,\n",
    "            path=self.get_path(),\n",
    "            dataset=True,\n",
    "            mode=self.mode,\n",
    "            compression=\"snappy\",\n",
    "            dtype=dtype,\n",
    "            partition_cols=self.partition_cols,\n",
    "        )\n",
    "\n",
    "    def set_dtype(self, dtype: Dict[str, str]) -> None:\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def set_partition_cols(self, partition_cols: List[str]) -> None:\n",
    "        self.partition_cols = partition_cols\n",
    "\n",
    "    def _write_text(self, obj: Union[dict, list, str]) -> None:\n",
    "        if isinstance(obj, dict) or isinstance(obj, list):\n",
    "            new_obj = json.dumps(obj)\n",
    "        else:\n",
    "            new_obj = obj\n",
    "        resource = self.client.resource\n",
    "        s3object = resource.Object(self.bucket, self.key)\n",
    "        if isinstance(new_obj, io.TextIOWrapper):\n",
    "            if new_obj.mode == \"rb\":\n",
    "                new_obj = new_obj.read()\n",
    "            elif new_obj.mode == \"r\":\n",
    "                new_obj = new_obj.read().encode()\n",
    "            else:\n",
    "                raise ValueError(\"Invalid TextIOWrapper mode\")\n",
    "            s3object.put(Body=new_obj, ContentType=self.content_type)\n",
    "        elif isinstance(new_obj, str):\n",
    "            s3object.put(Body=new_obj.encode(), ContentType=self.content_type)\n",
    "\n",
    "    def write(self, obj: Any) -> None:\n",
    "        if self.format == \"parquet\" and isinstance(obj, DataFrame):\n",
    "            self._write_parquet(obj)\n",
    "        elif self.format == \"json\" and (\n",
    "            isinstance(obj, dict) or isinstance(obj, list) or isinstance(obj, str)\n",
    "        ):\n",
    "            self.content_type = \"application/json\"\n",
    "            self._write_text(obj)\n",
    "        elif self.format == \"text\" and isinstance(obj, str):\n",
    "            self.content_type = \"plain/text\"\n",
    "            self._write_text(obj)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Object and format combination not implemented\")\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, client: S3Client, config: Dict[str, str]):\n",
    "        bucket = config[\"s3_sink_bucket\"]\n",
    "        key = config[\"s3_sink_key\"]\n",
    "        file_format = config[\"format\"]\n",
    "        mode = config.get(\"mode\", \"\")\n",
    "        return cls(client=client, bucket=bucket, key=key, format=file_format, mode=mode)\n",
    "\n",
    "    def read(self) -> Union[DataFrame, str]:\n",
    "        if self.format == \"parquet\":\n",
    "            return self._read_parquet()\n",
    "        elif self.format == \"json\":\n",
    "            return self._read_text()\n",
    "        elif self.format == \"text\":\n",
    "            return self._read_text()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Format not implemented\")\n",
    "\n",
    "    def _read_text(self) -> str:\n",
    "        resource = self.client.resource\n",
    "        obj = resource.Object(self.bucket, self.key)\n",
    "        obj_response = obj.get()\n",
    "        content: bytes = obj_response[\"Body\"].read()\n",
    "        content_decoded = content.decode()\n",
    "        return content_decoded\n",
    "\n",
    "    def _read_parquet(self) -> DataFrame:\n",
    "        path = self.get_path() + \"/\"\n",
    "        frame = s3.read_parquet(path=path, dataset=True)\n",
    "        if isinstance(frame, DataFrame):\n",
    "            return frame\n",
    "        else:\n",
    "            raise ValueError(\"Path is incorrect\", frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f7b05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DateColumn:\n",
    "    name: str\n",
    "    value: datetime\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, date_col: Optional[Dict[str, Any]]):\n",
    "        if date_col is None:\n",
    "            return None\n",
    "        name: str = date_col[\"name\"]\n",
    "        value: datetime = date_col[\"value\"]\n",
    "        return cls(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a538b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DateTable:\n",
    "    created_at: Optional[DateColumn]\n",
    "    updated_at: Optional[DateColumn]\n",
    "    deleted_at: Optional[DateColumn]\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, date_dict: Dict[str, Dict[str, Any]]):\n",
    "        return cls(\n",
    "            DateColumn.parse(date_dict.get(\"created_at\")),\n",
    "            DateColumn.parse(date_dict.get(\"updated_at\")),\n",
    "            DateColumn.parse(date_dict.get(\"deleted_at\")),\n",
    "        )\n",
    "\n",
    "    def _as_dict(self):\n",
    "        as_dict = {\n",
    "            \"created_at\": self.created_at,\n",
    "            \"updated_at\": self.updated_at,\n",
    "            \"deleted_at\": self.deleted_at,\n",
    "        }\n",
    "        valid_as_dict = {k: v for k, v in as_dict.items() if v is not None}\n",
    "        return valid_as_dict\n",
    "\n",
    "    def _as_list(self):\n",
    "        as_list = [self.created_at, self.updated_at, self.deleted_at]\n",
    "        valid_as_list = [x for x in as_list if x is not None]\n",
    "        return valid_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3011ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableConfig:\n",
    "    base_query: str = \"select * from {table}\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        key: str,\n",
    "        created_at: Optional[str] = None,\n",
    "        updated_at: Optional[str] = None,\n",
    "        deleted_at: Optional[str] = None,\n",
    "        datetime_columns: Optional[List[str]] = None,\n",
    "    ) -> None:\n",
    "        self.name = name\n",
    "        self.key = key\n",
    "        self.created_at = created_at\n",
    "        self.updated_at = updated_at\n",
    "        self.deleted_at = deleted_at\n",
    "        self.datetime_columns = datetime_columns\n",
    "        '''is_full_refresh = (\n",
    "            created_at is None and updated_at is None and deleted_at is None\n",
    "        )'''\n",
    "        self.is_full_refresh = True #is_full_refresh\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"\"\"TableConfig(\n",
    "                    name={},\n",
    "                    key={},\n",
    "                    created_at={},\n",
    "                    updated_at={},\n",
    "                    deleted_at={},\n",
    "                    datetime_columns={},\n",
    "                    is_full_refresh={}\n",
    "                    )\"\"\".format(\n",
    "            self.name,\n",
    "            self.key,\n",
    "            self.created_at,\n",
    "            self.updated_at,\n",
    "            self.deleted_at,\n",
    "            self.datetime_columns,\n",
    "            self.is_full_refresh,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, config: Dict[str, Any]):\n",
    "        (name, key, created_at, updated_at, deleted_at, datetime_columns) = (\n",
    "            config[\"name\"],\n",
    "            config[\"key_column\"],\n",
    "            config.get(\"created_at\"),\n",
    "            config.get(\"updated_at\"),\n",
    "            config.get(\"deleted_at\"),\n",
    "            config.get(\"datetime_columns\"),\n",
    "        )\n",
    "        return cls(name, key, created_at, updated_at, deleted_at, datetime_columns)\n",
    "\n",
    "    def make_query(self, max_dates: DateTable):\n",
    "        formatted_query = self.base_query.format(table=self.name)\n",
    "        formatted_where = self._make_query_where(max_dates)\n",
    "        if formatted_where is not None:\n",
    "            final_sql = formatted_query + \" \" + formatted_where\n",
    "        else:\n",
    "            final_sql = formatted_query\n",
    "        return final_sql\n",
    "\n",
    "    def _make_query_where(self, max_dates: DateTable) -> Optional[str]:\n",
    "        dates = max_dates._as_dict()\n",
    "        if len(dates) > 0:\n",
    "            max_dates_list = [\n",
    "                \"(date({name}) >= date('{value}'))\".format(\n",
    "                    name=dates[k].name, value=dates[k].value.isoformat()\n",
    "                )\n",
    "                for k in dates\n",
    "            ]\n",
    "            final_sql_where = \"where \" + \" or \".join(max_dates_list)\n",
    "            return final_sql_where\n",
    "\n",
    "    def _dates_as_dict(self):\n",
    "        as_dict = {\n",
    "            \"created_at\": self.created_at,\n",
    "            \"updated_at\": self.updated_at,\n",
    "            \"deleted_at\": self.deleted_at,\n",
    "        }\n",
    "        valid_as_dict = {k: v for k, v in as_dict.items() if v is not None}\n",
    "        return valid_as_dict\n",
    "\n",
    "    def _get_max_dates(self, frame: DataFrame) -> DateTable:\n",
    "        dates_as_dict = self._dates_as_dict()\n",
    "        max_dates: Dict[str, Dict[str, Any]] = {}\n",
    "        for date_name, date_column in dates_as_dict.items():\n",
    "            series: Series = frame[date_column]\n",
    "            max_date: Timestamp = series.max()\n",
    "            max_date_datetime: datetime = max_date.to_pydatetime()\n",
    "            max_dates[date_name] = {\"name\": date_column, \"value\": max_date_datetime}\n",
    "        return DateTable.parse(max_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d8c64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplicationConfig:\n",
    "    tables: List[TableConfig]\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, config: List[Dict[str, Any]]):\n",
    "        tables = [TableConfig.parse(table_config) for table_config in config]\n",
    "        return cls(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6599a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AuthConfig:\n",
    "    protocol: str\n",
    "    host: str\n",
    "    port: str\n",
    "    username: str\n",
    "    password: str\n",
    "    dbname: str\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, config: Dict[str, str]):\n",
    "        (protocol, host, port, username, password, dbname) = (\n",
    "            config[\"protocol\"],\n",
    "            config[\"host\"],\n",
    "            config[\"port\"],\n",
    "            config[\"username\"],\n",
    "            config[\"password\"],\n",
    "            config[\"dbname\"],\n",
    "        )\n",
    "        return cls(protocol, host, port, username, password, dbname)\n",
    "\n",
    "    def _as_dict(self):\n",
    "        return {\n",
    "            \"protocol\": self.protocol,\n",
    "            \"host\": self.host,\n",
    "            \"port\": self.port,\n",
    "            \"username\": self.username,\n",
    "            \"password\": self.password,\n",
    "            \"dbname\": self.dbname,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cfeaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JobConfig:\n",
    "    bucket: str\n",
    "    base_path: str\n",
    "    prefix: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1f4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplicationResult:\n",
    "    table_config:TableConfig\n",
    "    success:bool\n",
    "\n",
    "    def make_body(self) -> Dict[str,Any]:\n",
    "        body = {\n",
    "            \"name\": self.table_config.name,\n",
    "            \"succeeded\": self.success,\n",
    "            \"full_refresh\": self.table_config.is_full_refresh,\n",
    "                }\n",
    "        return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014791f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuccessResult(ReplicationResult):\n",
    "    table_config: TableConfig\n",
    "    old_length: int\n",
    "    new_length: int\n",
    "    max_dates: DateTable\n",
    "    schema_change: bool\n",
    "    success:bool\n",
    "\n",
    "    def __init__(self, table_config:TableConfig, old_length:int,\n",
    "            new_length:int, max_dates:DateTable, schema_change:bool,\n",
    "            success:bool):\n",
    "        self.old_length = old_length\n",
    "        self.new_length = new_length\n",
    "        self.max_dates = max_dates\n",
    "        self.schema_change = schema_change\n",
    "        super().__init__(table_config, success)\n",
    "\n",
    "    def make_body(self) -> Dict[str, Any]:\n",
    "        body = {\n",
    "            \"name\": self.table_config.name,\n",
    "            \"succeeded\": self.success,\n",
    "            \"old_length\": self.old_length,\n",
    "            \"new_length\": self.new_length,\n",
    "            \"query\": self.table_config.make_query(self.max_dates),\n",
    "            \"schema_chaged\": self.schema_change,\n",
    "            \"max_dates\": self.max_dates,\n",
    "            \"full_refresh\": self.table_config.is_full_refresh,\n",
    "        }\n",
    "        return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7a3e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FailureResult(ReplicationResult):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b01d332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplicationSet:\n",
    "    results: List[ReplicationResult]\n",
    "\n",
    "    def make_report(self):\n",
    "        body = [x.make_body() for x in self.results]\n",
    "        return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "831c3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplicationClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        job_config: JobConfig,\n",
    "        replication_config: ReplicationConfig,\n",
    "        auth_config: AuthConfig,\n",
    "    ) -> None:\n",
    "        self.replication = replication_config\n",
    "        self.auth = auth_config\n",
    "        self.job = job_config\n",
    "        self.engine = self._get_db_engine()\n",
    "        self.s3_client = S3Client()\n",
    "\n",
    "    def _get_db_engine(self) -> Engine:\n",
    "        db_url = \"{protocol}://{username}:{password}@{host}:{port}/{dbname}\".format(\n",
    "            **self.auth._as_dict()\n",
    "        )\n",
    "        db_engine = create_engine(db_url)\n",
    "        return db_engine\n",
    "\n",
    "    def _get_sink_key(self, table_config: TableConfig) -> str:\n",
    "        key = f\"{self.job.base_path}/{self.job.prefix}{table_config.name}\"\n",
    "        return key\n",
    "\n",
    "    def get_sink(self, table_config: TableConfig) -> S3Sink:\n",
    "        key = self._get_sink_key(table_config)\n",
    "        bucket = self.job.bucket\n",
    "        mode = \"overwrite\" if table_config.is_full_refresh else \"append\"\n",
    "        sink = S3Sink(self.s3_client, bucket, key, \"parquet\", mode)\n",
    "        return sink\n",
    "\n",
    "    def load_data(self, table_config: TableConfig, max_dates: DateTable) -> DataFrame:\n",
    "        sql = table_config.make_query(max_dates)\n",
    "        frame = read_sql(sql, self.engine)\n",
    "        frame = self.convert_dates(table_config, frame)\n",
    "        return frame\n",
    "\n",
    "    def convert_dates(\n",
    "        self, table_config: TableConfig, initial_frame: DataFrame\n",
    "    ) -> DataFrame:\n",
    "        frame = initial_frame.copy()\n",
    "        datetime_cols = (\n",
    "            table_config.datetime_columns\n",
    "            if table_config.datetime_columns is not None\n",
    "            else []\n",
    "        )\n",
    "        date_cols = datetime_cols + list(table_config._dates_as_dict().values())\n",
    "        if len(date_cols) > 0:\n",
    "            print(\"Converting datetime columns\")\n",
    "            frame = convert_dates(frame, date_cols)\n",
    "        return frame\n",
    "\n",
    "    def load_existing_data(\n",
    "        self, table_config: TableConfig, sink: S3Sink\n",
    "    ) -> Optional[DataFrame]:\n",
    "        if sink.exists():\n",
    "            frame = sink.read()\n",
    "            if isinstance(frame, DataFrame):\n",
    "                frame = self.convert_dates(table_config, frame)\n",
    "                return frame\n",
    "            else:\n",
    "                raise RuntimeError(\"Return should be a DataFrame\")\n",
    "        return None\n",
    "\n",
    "    def _compare_schema(self, old: Optional[DataFrame], new: DataFrame):\n",
    "        if old is None:\n",
    "            return True\n",
    "        old_cols = old.columns\n",
    "        new_cols = new.columns\n",
    "        is_same_schema = sorted(old_cols) == sorted(new_cols)\n",
    "        return is_same_schema\n",
    "\n",
    "    def process_table(self, table_config: TableConfig) -> SuccessResult:\n",
    "        print(f\"Processing table {table_config}\")\n",
    "        sink = self.get_sink(table_config)\n",
    "        #existing_frame = self.load_existing_data(table_config, sink)\n",
    "        '''if existing_frame is not None:\n",
    "            print(\"Existing data found\")\n",
    "            max_dates: DateTable = table_config._get_max_dates(existing_frame)\n",
    "        else:'''\n",
    "        print(\"No existing data\")\n",
    "        max_dates = DateTable(None, None, None)\n",
    "        new_frame = self.load_data(table_config, max_dates)\n",
    "        #has_same_schema = self._compare_schema(existing_frame, new_frame)\n",
    "        if not new_frame.empty:\n",
    "            print(\"Writting frame\")\n",
    "            sink.write(new_frame)\n",
    "        else:\n",
    "            print(\"Empty frame\")\n",
    "        result = SuccessResult(\n",
    "            table_config,\n",
    "            #len(existing_frame) if existing_frame is not None else 0,\n",
    "            0,\n",
    "            len(new_frame),\n",
    "            max_dates,\n",
    "            #not has_same_schema,\n",
    "            False,\n",
    "            True\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def replicate(self) -> ReplicationSet:\n",
    "        print(\"Starting Replication\")\n",
    "        tables = self.replication.tables\n",
    "        results: List[ReplicationResult] = []\n",
    "        for table_config in tables:\n",
    "            try:\n",
    "                result = self.process_table(table_config)\n",
    "            except KeyboardInterrupt as e:\n",
    "                raise e\n",
    "            except BaseException as e:\n",
    "                print(traceback.format_exc())\n",
    "                result = FailureResult(table_config, False)\n",
    "            print(result.make_body())\n",
    "            results.append(result)\n",
    "        return ReplicationSet(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f571fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Starting Job Run\")\n",
    "    #platfarm_configs = [CONFIG[\"platfarm\"], CONFIG[\"platfarm_cadastro\"], CONFIG[\"platfarm_credito_gestao\"], CONFIG[\"platfarm_credito_solicitacao\"]]\n",
    "    platfarm_configs = CONFIG[\"platfarm\"],\n",
    "    job_config = JobConfig(\n",
    "        CONFIG[\"s3\"][\"bucket_name\"],\n",
    "        CONFIG[\"s3\"][\"base_path\"],\n",
    "        CONFIG[\"s3\"][\"table_prefix\"],\n",
    "    )\n",
    "    for iten in platfarm_configs:\n",
    "        auth_config = read_json_valt(iten[\"auth\"])\n",
    "        auth_config = AuthConfig.parse(auth_config)\n",
    "        tables_config: List[Dict[str, Any]] = read_json_configs(iten[\"tables\"])\n",
    "        replication_config = ReplicationConfig.parse(tables_config)\n",
    "        client = ReplicationClient(job_config, replication_config, auth_config)\n",
    "        result = client.replicate()\n",
    "        print(result.make_report())\n",
    "        failures = [x for x in result.results if not x.success]\n",
    "        if len(failures) > 0:\n",
    "            print(\"Failures detected\")\n",
    "            sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fe56cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Job Run\n",
      "Starting Replication\n",
      "Processing table TableConfig(\n",
      "                    name=baixas,\n",
      "                    key=id,\n",
      "                    created_at=created_at,\n",
      "                    updated_at=updated_at,\n",
      "                    deleted_at=None,\n",
      "                    datetime_columns=['data', 'vencimento'],\n",
      "                    is_full_refresh=True\n",
      "                    )\n",
      "No existing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-dc6786326c0f>:12: DeprecationWarning: The raise_on_deleted parameter will change its default value to False in hvac v3.0.0. The current default of True will presere previous behavior. To use the old behavior with no warning, explicitly set this value to True. See https://github.com/hvac/hvac/pull/907\n",
      "  result = client.secrets.kv.v2.read_secret_version(mount_point='farm-bi-aws-glue-configs', path=path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime columns\n",
      "Writting frame\n",
      "{'name': 'baixas', 'succeeded': True, 'old_length': 0, 'new_length': 136185, 'query': 'select * from baixas', 'schema_chaged': False, 'max_dates': DateTable(created_at=None, updated_at=None, deleted_at=None), 'full_refresh': True}\n",
      "[{'name': 'baixas', 'succeeded': True, 'old_length': 0, 'new_length': 136185, 'query': 'select * from baixas', 'schema_chaged': False, 'max_dates': DateTable(created_at=None, updated_at=None, deleted_at=None), 'full_refresh': True}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
